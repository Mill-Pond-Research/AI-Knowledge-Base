# Prompting Techniques for Secure Code Generation: A Systematic Investigation

Authors: Catherine Tony, Nicolás E. Díaz Ferreyra, Markus Mutas, Salem Dhiff, Riccardo Scandariato
Hamburg University of Technology, Germany

## Table of Contents

1. Introduction
2. Related Work
3. Methodology for Systematic Literature Review
4. Prompting Techniques for Code Generation (RQ1)
5. Security Evaluation of Prompting Techniques: Methodology
6. Security Evaluation Results
7. Discussion
8. Threats to Validity
9. Conclusion
10. Replication Package

## 1. Introduction

<introduction>
This knowledge base document is based on a comprehensive study that explores the impact of different prompting techniques on the security of code generated by Large Language Models (LLMs). The document serves as a practical guide for researchers, developers, and professionals seeking to understand and improve the security aspects of AI-generated code.

The integration of LLMs into software development practices represents a significant shift in how code is generated and reviewed. This knowledge base aims to bridge the gap between prompting techniques and secure code generation, providing readers with foundational concepts, methodologies, and insights crucial for enhancing the security of LLM-generated code.

The document covers a wide range of topics, including:
- A systematic review of prompting techniques suitable for code generation
- Evaluation of these techniques for secure code generation using popular LLMs
- Analysis of security weaknesses in LLM-generated code
- Insights into the effectiveness of various prompting techniques in mitigating security vulnerabilities
</introduction>

### 1.1 Significance and Relevance

<significance>
The study addresses a critical concern in the field of AI-assisted software development: the security of code generated by Large Language Models. As LLMs become increasingly integrated into software development workflows, ensuring the security of the generated code is paramount. This research provides valuable insights into how different prompting techniques can influence the security aspects of LLM-generated code, making it highly relevant for both academic researchers and industry practitioners working on AI-assisted software development.
</significance>

### 1.2 Scope

<scope>
The scope of this study encompasses:
1. A systematic literature review to identify prompting techniques suitable for code generation
2. Adaptation of these techniques for secure code generation tasks
3. Evaluation of the adapted techniques using GPT-3, GPT-3.5, and GPT-4 models
4. Analysis of security weaknesses in the generated code using static analysis tools
5. Insights into the effectiveness of various prompting techniques in improving code security
</scope>

## 2. Related Work

This section provides an overview of existing research related to code generation using LLMs and the security aspects of LLM-generated code.

### 2.1 Code Generation Using LLMs

<related_work>
Several studies have evaluated the code generation capabilities of LLMs:

1. Hendrycks et al. (2021) evaluated GPT-2, GPT-3, and GPT-Neo using the APPS benchmark dataset.
2. Austin et al. (2021) explored program synthesis using models ranging from 244M to 137B parameters.
3. Xu et al. (2022) conducted a comprehensive assessment of various LLMs, including Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, CodeParrot, and PolyCoder.
4. Zeng et al. (2022) examined 8 LLMs for program understanding and generation tasks.

These studies primarily employed zero-shot or few-shot prompting techniques in their evaluations.
</related_work>

### 2.2 Security in LLM-Generated Code

<security_studies>
Several studies have investigated the security aspects of LLM-generated code:

1. Pearce et al. (2022) assessed code completions produced by GitHub Copilot and found that 40% contained security vulnerabilities.
2. Asare et al. (2023) compared C/C++ code generated by human developers against those generated by Copilot and found that Copilot is not significantly worse than humans in introducing vulnerabilities.
3. Jesse et al. (2023) examined if Codex and other LLMs generate simple, stupid bugs (SStuBs) and found that these models produce twice as many SStuBs as correct code.
4. Sandoval et al. (2023) conducted a user study on the security implications of LLM code assistants and found that participants using AI assistants introduced security-critical bugs at a rate no higher than 10% compared to the control group.
5. Perry et al. (2023) observed that participants utilizing AI assistants were prone to generating insecure solutions more often than those without AI assistance in four out of five tasks.
</security_studies>

<research_gap>
Despite the extensive research in the domain of code generation by LLMs, there is a lack of papers that explore various prompting techniques other than zero-shot and few-shot prompting to enhance the code generation capabilities of LLMs. Additionally, studies so far do not thoroughly explore the impact of prompting techniques to improve the security of the code generated by the LLMs. This underscores the need for further research to identify such techniques that can improve the secure code generation capabilities of LLMs.
</research_gap>

## 3. Methodology for Systematic Literature Review

This section outlines the methodology used to conduct the systematic literature review (SLR) to identify prompting techniques suitable for code generation.

### 3.1 Search Strategy

<search_strategy>
The authors used the Publish or Perish tool to retrieve papers from Google Scholar. The search query employed was:

"prompt* AND (engineer* OR pattern* OR technique*) AND (language model* OR pre-trained model* OR llm* OR ptm*)"

The search was conducted in October 2023, and results were examined in their ranked order.
</search_strategy>

### 3.2 Inclusion and Exclusion Criteria

<inclusion_criteria>
Inclusion Criteria:
IC1: Paper deals with prompting LLMs using one or more techniques
IC2: Paper is published since 2018
IC3: Paper is written in English
</inclusion_criteria>

<exclusion_criteria>
Exclusion Criteria:
EC1: Paper does not introduce new prompting techniques to query LLM
EC2: Paper deals with the generation of anything other than text and code (e.g., image, speech, and video data)
EC3: Paper that presents prompting techniques that cannot be used for generation tasks
EC4: Paper that presents automated prompt optimization techniques and frameworks
EC5: Paper that presents prompting technique for attacking the model
EC6: Out of scope (e.g., techniques for medical science)
</exclusion_criteria>

### 3.3 Screening Process

<screening_process>
The review process was done in two screening steps:
1. First screening: Titles and abstracts were examined to determine relevance.
2. Second screening: Full papers were reviewed to assess if they fit the criteria.

The authors reached saturation at the mark of 358 search results, as no new papers passed the first screening process within over 100 results before that point.
</screening_process>

### 3.4 Snowballing

<snowballing>
To ensure comprehensive coverage, the authors performed 3 rounds of backward snowballing. They went through the references of the selected papers iteratively following the same two-step screening process until no new papers were obtained. This process yielded 5 additional relevant papers, bringing the total number of relevant papers to 13.
</snowballing>

### 3.5 Knowledge Extraction

<knowledge_extraction>
Each final paper that introduced a prompting technique suitable for code generation was examined in detail. The primary objective was to extract the techniques themselves and pinpoint their key features. The authors performed a lightweight thematic analysis with open coding to interpret patterns or themes within the data.

The extracted information included:
- Details of the prompting techniques
- LLMs on which the technique was tested
- Specific tasks used for evaluation
- Datasets employed for evaluation
- Year of publication, venue, and citation count
</knowledge_extraction>

## 4. Prompting Techniques for Code Generation (RQ1)

This section presents an overview of the prompting techniques identified from the systematic literature review that are deemed suitable for code-generation tasks.

### 4.1 Overview of Selected Papers

<selected_papers>
The authors identified 15 distinct techniques designed for textual content generation with potential applicability to code-generation tasks from 13 papers. Key information about these papers includes:

- 10 out of 13 papers have undergone peer review
- The remaining papers have received at least 48 citations
- Except for two papers, all have conducted experimental validation of their introduced prompting techniques
- Only two papers have evaluated their techniques specifically for code generation tasks
- Ten out of eleven papers that conducted experimental validation utilize OpenAI models
</selected_papers>

### 4.2 Classification of Prompting Techniques

The authors classified the identified techniques into 5 different categories based on their strategic design:

1. Root Techniques
2. Refinement-based Techniques
3. Decomposition-based Techniques
4. Reasoning-based Techniques
5. Priming Techniques

#### 4.2.1 Root Techniques

<root_techniques>
These are the foundational and most popular techniques based on which more advanced techniques are built. They include:

1. Zero-shot: The model is asked to perform a task without task-specific training or examples at the time of inference.
2. One-shot/Few-shot: The model is given one or a few input-output examples before providing the final input for which it is expected to produce the output.
</root_techniques>

#### 4.2.2 Refinement-based Techniques

<refinement_techniques>
These techniques focus on improving, refining, or iterating the model outputs. They include:

1. Recursive Criticism and Improvement (RCI): This technique involves a two-step process where the LLM analyzes and critiques its current response, then rectifies the identified issues and revises its output accordingly.
2. Self-refine: Similar to RCI, this technique uses 2 steps called feedback and refine in addition to an initial output generation step.
3. Progressive Hint: This technique iteratively refines the output from the LLM by providing increasingly informative hints in each iteration.
</refinement_techniques>

#### 4.2.3 Decomposition-based Techniques

<decomposition_techniques>
These techniques break down complex tasks or prompts into simpler, more manageable pieces. They include:

1. Least-to-most: This technique is executed in two stages: decomposition and sub-problem solving.
2. Self-planning: Specifically designed for code generation problems, this approach is carried out in two phases: planning and implementation.
</decomposition_techniques>

#### 4.2.4 Reasoning-based Techniques

<reasoning_techniques>
These techniques guide the model to employ and demonstrate logical reasoning for generating responses. They include:

1. Chain-of-Thought (CoT): The LLM is compelled to produce a sequence of intermediary logical reasoning steps in natural language, culminating in the solution to the presented problem.
2. Zero-shot CoT: This approach addresses the limitations of the CoT approach, which requires task-specific reasoning examples.
3. Self-consistency/Complexity-based: These techniques are built on top of the CoT technique and use a sample-and-marginalize decoding strategy to generate more reliable output.
4. Few-shot with Explanation: This technique uses few-shot input-output examples with a task instruction with additional explanations for each of the examples.
</reasoning_techniques>

#### 4.2.5 Priming Techniques

<priming_techniques>
These techniques are designed to pre-program LLMs before prompting them with a task. They include:

1. Persona pattern/Memetic proxy: This approach involves asking the model to respond from a specific viewpoint or using a character or scenario as a stand-in for the requirements the LLM needs to fulfill when generating a response.
</priming_techniques>

<rq1_answer>
RQ1: The study identified 15 prompting techniques that can be used for code generation. They are zero-shot, one-shot, few-shot, RCI, self-refine, progressive hint, least-to-most, self-planning, CoT, zero-shot CoT, self-consistency, few-shot with explanation, persona pattern and memetic proxy prompting. These techniques are organized into 5 categories based on their common characteristics. They are root, refinement-based, decomposition-based, reasoning-based, and priming techniques.
</rq1_answer>

## 5. Security Evaluation of Prompting Techniques: Methodology

This section outlines the methodology used to evaluate the impact of different prompting techniques on the security of code generated by LLMs.

### 5.1 Dataset and Models

<dataset>
The authors used the LLMSecEval dataset, which consists of 150 NL prompts covering 18 of the Top 25 CWEs (Common Weakness Enumeration) from 2021. Each coding task is designed to lead to a code that is potentially vulnerable to one of the 18 CWEs if a naive implementation is used.
</dataset>

<models>
The evaluation was conducted using three OpenAI models:
1. GPT-3 (text-davinci-002)
2. GPT-3.5 (gpt-3.5-turbo)
3. GPT-4 (gpt-4-1106-preview)

These models were chosen due to their strong capabilities in both natural language processing and code generation, as well as their widespread usage in prompt engineering research.
</models>

### 5.2 Selection of Prompting Techniques

The authors conducted an initial screening to decide the suitability of prompting techniques for a more detailed analysis of their impact on generating secure code.

<qualification_criteria>
Qualification Criteria: The technique should be non-demonstrative in nature, i.e., it should not involve providing input-output examples.
</qualification_criteria>

<pre_study>
Pre-study: The authors used five randomly selected NL coding tasks from the LLMSecEval dataset and generated code using GPT-3 employing the techniques that met the qualification criteria.
</pre_study>

### 5.3 In-depth Analysis of Code Security

The authors followed these steps for the in-depth analysis:

1. Prompt Template Adaptation and Code Generation
2. Code Validity Analysis
3. Code Security Analysis

#### 5.3.1 Prompt Template Adaptation and Code Generation

<prompt_adaptation>
The authors tailored the prompting techniques to create prompt templates for secure code generation. This customization involved modifying:
1. Task instruction
2. Task input
3. (Optional) response trigger phrases

Once the prompt templates were adapted, the authors systematically generated code utilizing all three LLMs employing these templates.
</prompt_adaptation>

#### 5.3.2 Code Validity Analysis

<code_validity>
The validity of the code was characterized by 2 factors:
1. Task alignment: Ensuring the generated code meets the functional requirements outlined in the coding task description.
2. Code completeness: Verifying if the specified functionality in the task description is completely implemented in the code.
</code_validity>

#### 5.3.3 Code Security Analysis

<security_analysis>
The authors utilized Bandit, a static analysis tool specifically engineered to detect security weaknesses in Python code. Bandit examines the code and provides a report detailing:
- Number of weaknesses
- Descriptions of weaknesses
- Associated CWE IDs
- Severity levels
- Confidence levels

To gauge the reliability of Bandit's results, the authors manually verified the outcomes for a small subset (10%) of the code snippets produced by GPT-3.
</security_analysis>

## 6. Security Evaluation Results

This section presents the results of the security evaluation of code generated by GPT-3, GPT-3.5, and GPT-4 using different prompting techniques.

### 6.1 Selected Prompting Techniques for In-depth Security Analysis

<selected_techniques>
After the initial screening, the following prompting techniques were selected for in-depth analysis:
1. Zero-shot
2. Zero-shot CoT
3. RCI (Recursive Criticism and Improvement)
4. Persona/Memetic proxy
</selected_techniques>

### 6.2 Adapted Prompt Templates

<adapted_templates>
The authors adapted the selected prompting techniques for secure code generation tasks. For example, the zero-shot prompting technique was tested with four distinct prompt variations:
1. Baseline
2. Naive-secure
3. CWE-specific
4. Comprehensive
</adapted_templates>

### 6.3 Security in LLM-generated Code (RQ2)

<security_results>
Key findings from the security evaluation include:

1. The baseline prompt from the zero-shot family of prompting techniques was used as the base against which the effectiveness of various prompting techniques was measured.

2. Zero-shot prompt variations (naive-secure, CWE-specific, and comprehensive) showed evidence of a reduction in the number of overall weaknesses, rate, and weakness density compared to the baseline prompt.

3. RCI technique yielded the least average number of weaknesses in code generated by GPT-3.5 (0.42 weakness per code and 0.021 weakness density) and GPT-4 (0.27 weakness per code and 0.011 weakness density).

4. For GPT-3, even though simple zero-shot prompting yields the best results in terms of total number and rate of weaknesses, RCI seems to deliver the least number of weaknesses per LOC (0.029 weakness density).

5. The persona/memetic proxy approach led to the highest average number of security weaknesses among all the evaluated prompting techniques excluding the baseline prompt.
</security_results>

<statistical_analysis>
Statistical tests were performed to determine the significance of the results:

1. A Kruskall Wallis test was conducted on the weakness density metric for each LLM.
2. A Dunn's Post-Hoc test with Bonferroni correction was performed to further understand the results.

The tests revealed significant differences in the outcomes of GPT-4, particularly for the RCI technique.
</statistical_analysis>

<detected_weaknesses>
The four most commonly observed weaknesses include:
1. CWE-78 (Improper Neutralization of Special Elements used in an OS Command)
2. CWE-259 (Use of Hard-coded Passwords)
3. CWE-94 (Improper Control of Generation of Code)
4. CWE-330 (Use of Insufficiently Random Values)

Employing RCI led to a noticeable reduction in the occurrences of CWE-94, CWE-259, and CWE-330 within GPT-3.5 and GPT-4.
</detected_weaknesses>

<rq2_answer>
RQ2: Among the prompting techniques examined for secure code generation using our prompt templates, RCI which is a refinement-based technique exhibited the most favorable performance, particularly evident with GPT-3.5 and GPT-4. In the case of GPT-3, even though RCI delivers the least weakness density, zero-shot prompting yields the best results in terms of the weakness count and rate. Persona/memetic proxy on the other hand demonstrated the poorest performance, resulting in the highest number of security weaknesses across code generated by all three LLMs.
</rq2_answer>

## 7. Discussion

This section provides a more detailed analysis of the results, aiming to obtain a deeper understanding of the security aspects surrounding Python code generated by the LLMs.

### 7.1 Effect of Prompting Techniques on Security

<zero_shot_impact>
Zero-shot Prompting: 
- Simple addition of the term "secure" to the prompt led to a reduction in the average weakness density of the generated code by 28.15%, 37.03%, and 42.85% for GPT-3, GPT-3.5, and GPT-4, respectively.
- CWE-specific prompt variant achieved a reduction of 64.07% and 59.18% for GPT-3 and GPT-4, respectively compared to the baseline prompts.
- Comprehensive prompt variant reduced the weakness density by 31.57% and 20% for GPT-3.5 for GPT-4 respectively compared to that of CWE-specific prompts.
</zero_shot_impact>

<zero_shot_cot_impact>
Zero-shot CoT Prompting:
- Achieved a reduction in the weakness density by 56.31%, 20.37%, and 42.85% in the code generated by GPT-3 GPT-3.5, and GPT-4 respectively, compared to the baseline prompt.
- While it demonstrated superiority over the zero-shot prompting technique for GPT-3.5 in terms of weakness count and rate, there are zero-shot prompt variations that outperform this method across all models when considering weakness density.
</zero_shot_cot_impact>

<rci_impact>
RCI Prompting:
- Consistently yields the best results for both GPT-3.5 and GPT-4.
- For GPT-4, RCI managed to achieve a significant reduction of 77.55% in the average weakness density compared to the baseline prompt.
- Resulted in statistically significant reductions in weakness density compared to other prompt types.
</rci_impact>

<persona_impact>
Persona/Memetic Proxy:
- Consistently performed the worst in terms of weakness count, rate, and density by all the LLMs.
- For GPT-3.5, the weakness rate and density obtained for this technique is more than the baseline prompt.
</persona_impact>

### 7.2 Prominent CWEs in LLM-generated Code

<cwe_78>
CWE-78 (Improper Neutralization of Special Elements used in an OS Command):
- One of the most frequently recorded weaknesses across the code generated by all three LLMs.
- Often materializes in the form of an operating system command initiating a process with a partial executable path or when a subprocess.run() command is invoked using user-provided input.
- The adoption of different prompting techniques does not significantly diminish the frequency of this weakness in the generated code by any of the three models.
</cwe_78>

<cwe_259>
CWE-259 (Use of Hard-coded Passwords):
- Frequently materialized as static credentials embedded for login authentication and MySQL database connections for various operations.
- The RCI prompting technique notably reduced this vulnerability in GPT-3.5 (from 24 instances to 15) and GPT-4 (from 21 instances to 5).
</cwe_259>

<cwe_94>
CWE-94 (Improper Control of Generation of Code):
- Flagged by Bandit whenever a Flask application was executed in debug mode.
- The RCI technique reduced instances from 12 to 2 for GPT-3, 21 to 3 for GPT-3.5, and 54 to 3 for GPT-4.
</cwe_94>

<cwe_330>
CWE-330 (Use of Insufficiently Random Values):
- Occurs when less secure generators like 'random.random' or 'random.randint' are used to generate random values.
- In GPT-3.5 and GPT-4, both the comprehensive variant of zero-shot prompts and RCI prompts have notably diminished this vulnerability in code.
</cwe_330>

### 7.3 Changes in Coding Behavior

The authors observed several changes in coding behavior when using different prompting techniques:

<behavior_changes>
1. Addition of appropriate security measures: The most desirable coding behavior, where the model integrates suitable security measures into the generated code.

2. Addition of try-catch statements: A recurring pattern observed when prompted with techniques designed to include security considerations.

3. Addition of unnecessary security measures: Frequently observed when the models exhibit uncertainty regarding the appropriate security measures to be included.

4. Additional validation checks: Notably increased in code generated through the RCI prompting technique, especially in GPT-3.5 and GPT-4.

5. Security related comments: Some code generated by both GPT-3.5 and GPT-4 include warnings highlighting potential vulnerabilities.

6. Calls to undeclared/undefined secure methods: Observed in GPT-3.5 and GPT-4 where the generated Python snippet included calls to undeclared methods that did not exist within the code's scope.

7. Modification of method names: A pattern where method names in the generated code are prefixed with the term 'secure', particularly prevalent in code generated by GPT-3 and GPT-3.5.
</behavior_changes>

## 8. Threats to Validity

The authors acknowledge several limitations and potential threats to the validity of their study:

<construct_validity>
Construct Validity:
- Bandit, like many static analysis tools, may output false positive results. To address this, the authors performed a manual validation of Bandit output over a small sample of GPT-3-generated code snippets.
- The validity analysis of code responses was conducted by a single author, potentially introducing biases in the evaluation process.
- The prompting techniques were evaluated using prompt templates created by the authors, which might have influenced the results obtained from the LLMs.
</construct_validity>

<external_validity>
External Validity:
- The study evaluates security in Python code, which may affect the generalizability of results to other programming languages.
- The study was conducted only using OpenAI models (GPT-3, GPT-3.5, and GPT-4).
- The authors focused on prompting techniques that do not rely on demonstrative examples.
- The LLMSecEval dataset contains NL prompts for only 18 out of Top 25 CWEs of the year 2021.
</external_validity>

## 9. Conclusion

<conclusion>
The study provides several important insights into the use of prompting techniques for secure code generation using LLMs:

1. The prevalence of security weaknesses in code generated by LLMs when prompted with NL instructions is reaffirmed, with significant challenges stemming from CWE-78, CWE-259, CWE-94, and CWE-330.

2. Among the prompting techniques investigated, RCI (Recursive Criticism and Improvement), a refinement-based approach, exhibited notable effectiveness in preventing security weaknesses in LLM-generated code. It was particularly effective with GPT-4, reducing the average weakness density by 77.5% compared to baseline prompting.

3. Zero-shot prompting yielded surprisingly favorable outcomes considering its straightforward nature, performing better than zero-shot CoT and persona/memetic proxy yet falling short of RCI.

4. The persona/memetic proxy approach demonstrated the poorest performance, resulting in the highest number of security weaknesses across code generated by all three LLMs.

5. The study highlights the need for additional research to investigate refinement-based methods, like RCI, which leverage self-critiquing and improvement capabilities of LLMs to enhance security in LLM-generated code.

6. Future work can focus on exploring prompt optimization techniques, such as genetic algorithms and black-box tuning, to automatically identify optimal prompts for RCI and zero-shot techniques for secure code generation.
</conclusion>

## 10. Replication Package

<replication_package>
All data collected and generated in this study are available at https://figshare.com/s/1df4fcedde2901e76870. This repository contains:

1. Results of the literature review
2. Information on the prompting techniques that were removed from the final selection and the rationale behind this exclusion
3. Code generated by all 3 LLMs for the 7 prompt templates
4. Validity and security analysis results of the generated code
</replication_package>