# Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You

## Table of Contents

1. Introduction and Background
2. Related Work and Context
3. MAGBIG: Multilingual Assessment of Gender Bias in Image Generation
4. Experimental Protocol and Methodology
5. Generic Masculine Skews Gender Appearance in Generated Images
6. Gender Bias Magnification Across Languages at Large Scale
7. Discussion and Implications
8. Conclusion and Future Directions
9. Broader Impact and Ethical Considerations
10. Acknowledgments
11. References
12. Appendix and Supplementary Materials

## 1. Introduction and Background

The rapid advancement of artificial intelligence (AI) technologies has led to remarkable breakthroughs in various domains, with text-to-image (T2I) generation models emerging as a particularly impressive innovation. These models have demonstrated an unprecedented ability to create high-quality, diverse images from textual descriptions, opening up new possibilities for creative expression, design, and communication across language barriers. However, as with many AI technologies, these advancements come with inherent risks and challenges, particularly in the realm of bias and fairness.

This knowledge base document is based on a comprehensive study that delves into the complex interplay between multilingual capabilities in T2I models and the perpetuation of gender stereotypes. The research presented here is of critical importance as it sheds light on how the democratization of AI technologies through multilingual models may inadvertently amplify existing societal biases.

The authors begin with a crucial disclaimer:

<quote>This work includes stereotyped images as generated by the models. For evaluation purposes, we must adopt normative assumptions about desirable model outputs. We adopt the notion of equity between female- and male-appearing faces in professions, though other formulations are also plausible.</quote>

This statement underscores the ethical considerations and methodological challenges inherent in studying bias in AI systems. It acknowledges that the very act of evaluating bias requires making certain normative assumptions, which themselves may be subject to debate and cultural variation.

The study focuses on two key aspects of multilingual T2I models:

1. The extent to which these models perpetuate and potentially amplify gender stereotypes across different languages.
2. The effectiveness (or lack thereof) of prompt engineering techniques, particularly the use of gender-neutral language, in mitigating these biases.

To address these questions, the researchers introduce MAGBIG (Multilingual Assessment of Gender Bias in Image Generation), a novel benchmark designed specifically for evaluating gender bias in multilingual T2I models. This benchmark represents a significant contribution to the field, as it allows for a systematic and comprehensive assessment of bias across multiple languages and linguistic structures.

The importance of this research cannot be overstated. As T2I models become increasingly integrated into various applications, from creative tools to professional software, the biases they exhibit can have far-reaching consequences. These biases may reinforce harmful stereotypes, limit opportunities for certain groups, and contribute to the marginalization of already underrepresented communities. By examining these issues in a multilingual context, the study provides valuable insights into how linguistic diversity interacts with AI-generated content, potentially exacerbating or mitigating biases in ways that monolingual studies cannot capture.

Moreover, the research addresses a critical gap in the existing literature. While previous studies have examined bias in monolingual T2I models, the multilingual aspect introduces new complexities, such as the impact of grammatical gender in certain languages and the varying cultural contexts associated with different linguistic communities. By exploring these nuances, the study contributes to a more holistic understanding of bias in AI systems and the challenges of creating truly inclusive and fair technologies in a globalized world.

In the following sections, we will delve deeper into the methodology, findings, and implications of this groundbreaking research, providing a comprehensive resource for researchers, practitioners, and policymakers working at the intersection of AI, language, and social justice.

## 2. Related Work and Context

To fully appreciate the significance of this study, it is essential to understand the broader context of research on bias in AI systems, particularly in the domains of Natural Language Processing (NLP) and image generation. This section provides an overview of key related work, highlighting the evolution of the field and the gaps that the current study aims to address.

### 2.1 Gender Bias in NLP

The study of gender bias in NLP has a rich history, with researchers identifying and attempting to mitigate biases in various language models and applications. Some seminal works in this area include:

<quote>Bolukbasi et al. (2016) showed gender bias in static word embeddings and used mathematical properties of the vector space to reduce the bias. Caliskan, Bryson, and Narayanan (2017) introduced the Word Embedding Association Test (WEAT) which similarly demonstrates that human biases are reflected in associations between word embeddings.</quote>

These early studies were crucial in establishing that AI models can inherit and perpetuate societal biases present in their training data. They also laid the groundwork for various debiasing techniques, such as:

1. Post-processing methods that adjust learned word embeddings
2. Data augmentation techniques to balance gender representation in training corpora
3. Adversarial training approaches to remove gender information from intermediate representations

Subsequent research has expanded on these foundations, exploring bias in more complex language models and in various downstream tasks such as machine translation, coreference resolution, and sentiment analysis. For example:

- Zhao et al. (2018) examined gender bias in coreference resolution systems, showing how these systems tend to associate certain professions with specific genders.
- Stanovsky, Smith, and Zettlemoyer (2019) developed a challenge set for evaluating gender bias in machine translation, revealing how translation systems often default to masculine forms when gender is ambiguous in the source language.

These studies highlight the pervasive nature of gender bias in NLP systems and the ongoing challenges in addressing it effectively.

### 2.2 Grammatical Gender and Its Implications

A crucial aspect of the current study is its focus on multilingual models, which introduces additional complexities due to the varying treatment of gender across languages. The authors note:

<quote>When referring to people, the grammatical gender typically indicates the person's gender. Many languages also use the 'generic masculine': the masculine form can be used for any gender, whereas the feminine form is reserved for women.</quote>

This linguistic feature adds a layer of complexity to bias evaluation and mitigation strategies in multilingual contexts. Some key points to consider:

1. Languages with grammatical gender (e.g., Spanish, German, French) often require gender agreement between nouns, adjectives, and articles, making it more challenging to construct gender-neutral phrases.

2. The use of the generic masculine in many languages can lead to an overrepresentation of masculine forms in corpora, potentially skewing model outputs.

3. Some languages (e.g., English) have moved towards more gender-neutral language in recent years, while others maintain stronger gender distinctions in their grammar and usage.

Research in this area has shown that grammatical gender can influence cognitive processes and social perceptions. For instance, Sato et al. (2013) demonstrated that grammatical gender can affect the mental representations of objects, potentially influencing how people conceptualize and describe the world around them.

Understanding these linguistic nuances is crucial for developing effective strategies to mitigate gender bias in multilingual AI systems.

### 2.3 Biases in Text-to-Image Models

The emergence of powerful T2I models has brought renewed attention to issues of bias in AI-generated content. The authors cite several recent studies:

<quote>Despite their outstanding performance, previous work (Friedrich et al. 2023; Bianchi et al. 2023; Srinivasan and Bisk 2022; Bansal et al. 2022; Schramowski et al. 2023; Brack et al. 2023b) found T2I models to suffer severely from gender, racial, and other biases.</quote>

These studies have revealed various manifestations of bias in T2I models, including:

1. Stereotypical representations of occupations (e.g., depicting doctors as predominantly male and nurses as predominantly female)
2. Underrepresentation of certain racial and ethnic groups in generated images
3. Reinforcement of beauty standards and body image stereotypes
4. Biased depictions of social roles and activities based on gender, age, or ethnicity

The biases observed in T2I models often reflect and amplify societal biases present in the training data, which typically consists of large-scale image-text pairs scraped from the internet. This highlights the importance of critically examining the data sources and curation processes used in developing these models.

### 2.4 Benchmarking and Evaluation of T2I Models

As T2I models have advanced, so too have the methods for evaluating their performance. The authors note the existence of various benchmarks, but highlight a significant gap:

<quote>Yet, these benchmarks focus on English-only non-societal impacts but on image quality and fidelity. Recently, novel multilingual benchmarks (Lee et al. 2023; Ye et al. 2023; Saxon and Wang 2023) have been introduced that evaluate multilingual abilities for T2I models. But they again focus on non-societal impacts like general image quality and fidelity.</quote>

This observation underscores the need for benchmarks that specifically address societal impacts, such as bias and fairness, especially in multilingual contexts. Existing benchmarks typically focus on:

1. Image quality metrics (e.g., Fréchet Inception Distance, Inception Score)
2. Text-image alignment scores (e.g., CLIP score)
3. Diversity of generated images
4. Ability to handle complex prompts or rare concepts

While these aspects are important for assessing the technical capabilities of T2I models, they do not capture the nuanced social and ethical implications of the generated content. The development of MAGBIG addresses this gap by providing a comprehensive framework for evaluating gender bias across multiple languages and cultural contexts.

By situating their work within this broader research landscape, the authors demonstrate the novelty and significance of their contribution. The study builds upon existing knowledge of bias in NLP and image generation while extending these insights into the relatively unexplored territory of multilingual T2I models. This approach not only advances our understanding of bias in AI systems but also highlights the interconnected nature of language, culture, and technology in shaping the outputs of these powerful generative models.

## 3. MAGBIG: Multilingual Assessment of Gender Bias in Image Generation

The introduction of MAGBIG (Multilingual Assessment of Gender Bias in Image Generation) represents a significant contribution to the field of AI ethics and fairness. This novel benchmark is designed to systematically evaluate gender bias in multilingual text-to-image (T2I) models, addressing a critical gap in existing evaluation frameworks. Let's delve deeper into the key features and design principles of MAGBIG.

### 3.1 Prompt Diversity and Structure

MAGBIG comprises a carefully curated set of prompts designed to probe various aspects of gender representation in generated images. The authors state:

<quote>Our set of prompts contains 20 adjectives and 150 occupations. We categorize these as shown in Fig. 2.</quote>

This diversity in prompt types allows for a comprehensive assessment across different semantic domains. The inclusion of both adjectives and occupations is particularly important because:

1. Adjectives can reveal subtle biases in personality trait attribution based on gender.
2. Occupations often exhibit strong gender stereotypes and can highlight biases in professional representation.

The prompts are structured into three main types:

1. Adjective-based prompts: e.g., "a photo of an ambitious person"
2. Occupation-based prompts using nouns: e.g., "a photo of an accountant"
3. Occupation-based prompts using descriptions: e.g., "a person who manages finances for others as a profession"

This multi-faceted approach allows for the evaluation of bias across different linguistic constructions and levels of specificity.

### 3.2 Multilingual Coverage

One of the most innovative aspects of MAGBIG is its multilingual design. The benchmark covers nine languages:

<quote>To evaluate the extent to which grammatical gender affects image generation, MAGBIG includes languages with diverse gender systems (cf. Tab. 1).</quote>

The languages included are:

1. Arabic (ar)
2. Chinese (zh)
3. English (en)
4. French (fr)
5. German (de)
6. Italian (it)
7. Japanese (ja)
8. Korean (ko)
9. Spanish (es)

This selection of languages is particularly valuable because it encompasses:

- Languages with different writing systems (e.g., alphabetic, logographic)
- Languages from various language families (e.g., Indo-European, Sino-Tibetan, Afroasiatic)
- Languages with varying degrees of grammatical gender (e.g., gendered nouns, gendered pronouns, no grammatical gender)

This diversity allows for a nuanced analysis of how linguistic features interact with gender bias in T2I models across different cultural and linguistic contexts.

### 3.3 Gender-Neutral Formulations

MAGBIG incorporates a range of strategies for creating gender-neutral prompts, which is crucial for assessing the effectiveness of linguistic interventions in mitigating bias. These strategies include:

1. Using gender-neutral terms where possible (e.g., "person" instead of "man" or "woman")
2. Employing longer descriptions that avoid gendered nouns (e.g., "someone who teaches in a school" instead of "teacher")
3. Utilizing modern gender-neutral conventions specific to certain languages (e.g., the German "Gender Star" convention)

For example, the authors describe their approach to German:

<quote>As an example, we consider the German gender-star convention that merges the male ("Jurist") and female ("Juristin") versions with a star, where the star implies the inclusion of all genders beyond the binary male and female.</quote>

This attention to language-specific gender-neutral formulations is a key strength of MAGBIG, as it allows for the evaluation of bias mitigation strategies that are culturally and linguistically appropriate.

### 3.4 Scalability and Extensibility

While MAGBIG currently covers nine languages, its design principles and methodology can be extended to additional languages. The authors note:

<quote>We publish our pipeline that uses machine translation and several natural language processing tools to generate the translation automatically, thus enabling future extensions of the dataset.</quote>

This scalability is crucial for the ongoing development and refinement of multilingual T2I models, as it allows researchers to continually assess and address biases across an expanding range of languages and cultures.

### 3.5 Quantitative and Qualitative Assessment

MAGBIG is designed to facilitate both quantitative and qualitative analyses of gender bias:

1. Quantitative metrics: The benchmark allows for the calculation of various bias scores, such as the Mean Absolute Deviation (MAD) from gender parity in generated images.

2. Qualitative analysis: The diverse prompt set enables researchers to examine specific examples of generated images, providing insights into the nuanced ways that bias manifests across different languages and contexts.

This dual approach ensures a comprehensive evaluation that goes beyond simple numerical scores to provide a deeper understanding of the nature and extent of gender bias in T2I models.

### 3.6 Consideration of Intersectionality

While MAGBIG primarily focuses on gender bias, its design allows for the consideration of intersectional factors. By including a diverse range of occupations and attributes, the benchmark can potentially reveal how gender bias intersects with other forms of bias, such as those related to race, age, or socioeconomic status.

In conclusion, MAGBIG represents a significant advancement in the evaluation of bias in AI systems. Its comprehensive, multilingual approach provides researchers and practitioners with a powerful tool for assessing and addressing gender bias in T2I models. By offering a standardized benchmark that accounts for linguistic and cultural diversity, MAGBIG contributes to the development of more equitable and inclusive AI technologies that can serve diverse global communities.

## 4. Experimental Protocol and Methodology

The experimental protocol and methodology employed in this study are crucial for understanding the rigor and validity of the findings. The authors have developed a comprehensive approach to evaluating gender bias in multilingual text-to-image (T2I) models using the MAGBIG benchmark. Let's examine the key components of their methodology in detail.

### 4.1 General Setup

The authors outline a three-fold approach to assessing gender bias:

<quote>(1) We generated images for different prompts describing the groups of interest—here with prompts for multiple languages. (2) Next, we employed a classifier to investigate the generated images for bias, e.g. gender appearance. (3) In the last step, we accumulated previous results and evaluated this distribution to see if it, in fact, has any preference (bias) toward certain groups.</quote>

This systematic approach allows for a comprehensive evaluation of bias across different prompts, languages, and models. Let's break down each step:

1. Image Generation:
   - The study uses prompts from MAGBIG to generate images across multiple languages.
   - Two multilingual T2I models are evaluated: MultiFusion and AltDiffusion.
   - For each prompt, 100 images are generated, resulting in a large dataset for analysis.

2. Image Classification:
   - An automated classifier (FairFace) is used to assess the perceived gender of individuals in the generated images.
   - This step allows for quantitative analysis of gender representation across different prompts and languages.

3. Bias Evaluation:
   - The distribution of perceived genders in the generated images is analyzed to identify preferences or biases.
   - Statistical measures are employed to quantify the extent of bias in the model outputs.

This multi-step process ensures a thorough examination of bias, from the initial prompt to the final image analysis.

### 4.2 Bias Definition and Metrics

The authors provide a clear definition of fairness and bias in the context of their study:

<quote>For our evaluation, we define fairness as equity, meaning the absence of bias, in line with closely related work (Xu et al. 2018; Friedrich et al. 2023; Mehrabi et al. 2021; Bansal et al. 2022; Zhang et al. 2023). Equity can be described as an equal chance of all outcomes, in our particular case, regardless of demography or the train data.</quote>

This definition establishes a normative framework for evaluating bias, with the ideal outcome being an equal representation of genders across all prompts and languages.

To quantify bias, the study employs the Mean Absolute Deviation (MAD) score:

<quote>To measure fairness, we follow previous works (Cho, Zala, and Bansal 2023; Chuang et al. 2023) and calculate the MAD score. That is, we choose the absolute deviation from the normative assumption P(a), i.e. AD = |P(x) − P(a)|. Then, we average this score across all prompts resulting in the Mean Absolute Deviation: MAD = 1/|X| ∑(x∈X) |P(x) − P(a)|.</quote>

The MAD score provides a single, interpretable metric for assessing the overall level of bias in the model outputs. A MAD score of 0 would indicate perfect equity, while higher scores indicate greater levels of bias.

### 4.3 Evaluating Perceived Gender

The study uses an automated classifier to assess the perceived gender of individuals in the generated images:

<quote>To this end, we employ an image classifier, FairFace (Kärkkäinen and Joo 2021), to evaluate the massive gender imbalance in generated images in terms of perceived gender.</quote>

The use of FairFace is significant for several reasons:

1. Consistency: It allows for uniform evaluation across a large number of images.
2. Objectivity: It reduces potential human bias in gender classification.
3. Efficiency: It enables the analysis of a much larger dataset than would be feasible with manual annotation.

However, the authors acknowledge the limitations of this approach, particularly in terms of its binary classification of gender. This is an important consideration when interpreting the results and highlights the need for more nuanced approaches to gender representation in AI research.

### 4.4 Measuring Image Quality

To ensure a comprehensive evaluation, the study incorporates measures of image quality alongside bias assessment:

<quote>We use two measures for image quality: Text-to-image alignment and an attempt count, c100.</quote>

1. Text-to-Image Alignment:
   <quote>For text-to-image alignment (Hessel et al. 2021), we embed both the prompt text t and its generated image I with CLIP (Radford et al. 2021) in a vector representation (et, ei) and compute their cosine similarity—the text-to-image alignment— in this multimodal space cos(et, ei). A higher score is desirable.</quote>

   This measure ensures that the generated images are relevant to the input prompts, which is crucial for a fair assessment of bias.

2. Attempt Count (c100):
   This metric counts the number of attempts required to generate 100 images with visible faces for each prompt. It serves as a proxy for the model's understanding of the input and its ability to generate relevant images.

These quality measures are important because they help distinguish between bias issues and general model performance issues. For example, a low-quality model might produce random or irrelevant images, which could skew the bias assessment.

### 4.5 Models Evaluated

The study evaluates two multilingual T2I models:

1. MultiFusion:
   <quote>We investigate MultiFusion (Bellagente et al. 2023), which is reported to support: English (en), French (fr), German (de), Italian (it), and Spanish (es). Additionally, we found that it can also generate images for Arabic (ar) and Japanese (ja), although they are presumably out-of-distribution, i.e., they are not listed as part of the (pre-)training data.</quote>

2. AltDiffusion:
   <quote>Further, we consider AltDiffusion, (Ye et al. 2023) which officially supports nine languages: Arabic (ar), Chinese (zh), English (en), French (fr), Italian (it), Japanese (ja), Korean (ko), and Spanish (es). However, we found that it can also generate images for German (de).</quote>

The selection of these two models allows for a comparison of bias across different model architectures and language coverages. It also enables the exploration of how models perform on languages that were not explicitly part of their training data (out-of-distribution languages).

### 4.6 Statistical Analysis and Visualization

The study employs various statistical techniques and visualizations to present the results:

1. Bar charts showing MAD scores across languages and prompt types
2. Visualizations of text-to-image alignment scores
3. Qualitative examples of generated images for specific prompts

These analytical tools provide a comprehensive view of the bias landscape across different languages, models, and prompt types.

In conclusion, the experimental protocol and methodology employed in this study demonstrate a rigorous and multi-faceted approach to evaluating gender bias in multilingual T2I models. By combining quantitative metrics with qualitative analysis, and considering both bias and image quality, the researchers provide a nuanced and comprehensive assessment of the challenges and complexities involved in creating fair and equitable AI systems across linguistic and cultural boundaries.

## 5. Generic Masculine Skews Gender Appearance in Generated Images

This section of the study presents compelling evidence of gender bias in multilingual text-to-image (T2I) models, with a particular focus on how the use of generic masculine forms in certain languages affects the gender representation in generated images. The findings highlight the complex interplay between linguistic features and model outputs, revealing significant challenges in achieving fair and unbiased image generation across different languages.

### 5.1 Bias Across Languages

The authors provide striking visual evidence of gender bias in T2I models:

<quote>Fig. 3 shows example images for "accountant" in five languages (English, German, Italian, French, and Spanish) for two models (MultiFusion and AltDiffusion). The generated images show a clear tendency for this occupation to over-represent men. In all these languages, the models show mostly individuals that appear White and male.</quote>

This observation is significant for several reasons:

1. Consistency of Bias: The bias towards male representation is observed across multiple languages, suggesting a pervasive issue that transcends linguistic boundaries.

2. Intersectionality: The predominance of white male representations points to intersectional bias, where gender and racial biases compound.

3. Model Independence: The similar patterns observed in both MultiFusion and AltDiffusion indicate that this bias is not specific to a single model architecture or training approach.

These findings demonstrate that multilingual T2I models suffer from similar biases as their monolingual counterparts, despite the potential for diverse training data across languages.

### 5.2 Bias Differences Across Languages

While bias is observed across all languages, the study reveals important variations in how this bias manifests:

<quote>Fig. 1 shows that the generated images for German occupation prompts highly differ in gender appearance compared to English. One of the key problems is that some languages (like German) have grammatical gender and generally use the generic masculine when the gender is unspecified. In contrast to non-gendered languages (like English), this difference can impact image generation.</quote>

This observation highlights several crucial points:

1. Linguistic Features Matter: The presence of grammatical gender in a language can significantly influence the gender representation in generated images.

2. Generic Masculine Effect: Languages that use the generic masculine form for unspecified gender (e.g., German, Spanish, French) tend to produce more male-appearing images compared to languages without this feature (e.g., English).

3. Unintended Consequences: The use of grammatically correct, gender-neutral language (i.e., the generic masculine) can paradoxically lead to more biased image outputs.

These findings underscore the complexity of addressing bias in multilingual AI systems, as linguistic features that are neutral or inclusive in one context may contribute to bias in another.

### 5.3 Gender-Neutral Formulations

In an attempt to mitigate the bias introduced by generic masculine forms, the authors investigate the use of gender-neutral language:

<quote>As a possible remedy, we investigate the use of modern gender-neutral language that does not change the general formulation but does remove the generic masculine. As an example, we consider the German gender-star convention that merges the male ("Jurist") and female ("Juristin") versions with a star, where the star implies the inclusion of all genders beyond the binary male and female.</quote>

The results of this investigation are mixed:

<quote>The left side shows that the gender-neutral formulation can change gender appearance from a male- to a female-appearing lawyer. However, this effect only works reliably in one direction, as shown in Fig. 4b on the right.</quote>

These findings reveal several important points:

1. Potential for Mitigation: Gender-neutral language can, in some cases, help balance gender representation in generated images.

2. Asymmetric Effect: The effectiveness of gender-neutral formulations varies depending on the initial bias direction, suggesting a complex interaction between linguistic input and model behavior.

3. Limitations of Linguistic Interventions: While gender-neutral language shows some promise, it does not consistently resolve bias issues across all contexts.

The authors provide a technical explanation for the limited success of gender-neutral formulations:

<quote>This behavior may be attributed to the underlying method used to represent user prompts in T2I models (i.e., tokenization). The German "Jurist" will have a single token (e.g. tokenIDs=[50]). "Jurist*in" will be split into three tokens: one for the masculine stem, one for the star, and one for the feminine ending (e.g. tokenIDs=[50, 1042, 713]). The feminine "Juristin" will be split into two tokens, one for the masculine stem and one for the feminine ending (e.g. tokenIDs=[50, 713]). Consequently, the gender-neutral formulation uses the masculine prompt and emphasizes the female ending, and the star token is unlikely to have a big effect.</quote>

This explanation highlights the technical challenges in implementing effective bias mitigation strategies, as the internal representations of language in AI models may not align with human intuitions about gender-neutral language.

### 5.4 Implications and Challenges

The findings in this section have several important implications:

1. Cross-linguistic Variation: The study demonstrates that bias in T2I models can manifest differently across languages, necessitating language-specific evaluation and mitigation strategies.

2. Limitations of Simple Solutions: The mixed results of gender-neutral formulations suggest that simple linguistic interventions may not be sufficient to address deeply ingrained biases in AI systems.

3. Need for Holistic Approaches: Addressing bias in multilingual T2I models likely requires a combination of strategies, including diverse and balanced training data, improved model architectures, and careful consideration of linguistic and cultural factors.

4. Ethical Considerations: The study raises important questions about the ethical implications of deploying T2I models across different languages and cultures, given their potential to reinforce or amplify existing gender stereotypes.

In conclusion, this section provides compelling evidence of the complex relationship between language, gender representation, and AI-generated images. It highlights the challenges in creating truly unbiased multilingual T2I models and underscores the need for continued research and development in this area. The findings serve as a crucial reminder of the importance of considering linguistic and cultural diversity in the development and evaluation of AI technologies.

## 6. Gender Bias Magnification Across Languages at Large Scale

This section presents a comprehensive analysis of gender bias in multilingual text-to-image (T2I) models using the MAGBIG dataset. The findings reveal significant and persistent gender bias across languages and models, with important variations and nuances that have implications for the development and deployment of these technologies.

### 6.1 Prevalence of Gender Bias

The study finds substantial gender bias across all languages and models examined:

<quote>For all prompts across languages and models, we find significant gender bias as the red bars are far away from 0 (equity) and even from the dashed line (randomly biased reference). These results emphasize that even though concerns about (gender) bias have previously been raised, current T2I models still perpetuate these biases.</quote>

Key observations from this finding include:

1. Universality of Bias: The presence of significant bias across all languages suggests that this is a systemic issue in T2I models, not limited to specific languages or cultural contexts.

2. Persistence Despite Awareness: The fact that these biases persist despite increased awareness in the AI community highlights the challenging nature of addressing bias in complex AI systems.

3. Magnitude of Bias: The bias levels observed are not only statistically significant but also substantially higher than what would be expected from random variation, indicating strong and consistent gender stereotyping in the generated images.

### 6.2 Inconsistency Across Languages

An important nuance in the findings is the variation in bias manifestation across different languages:

<quote>Most importantly, one can observe that bias is inconsistent across languages. Some languages are more strongly affected by gender bias than others. However, there is no clear correlation with the degree to which they use grammatical gender (cf. Tab. 1).</quote>

This inconsistency has several implications:

1. Complex Linguistic Interactions: The lack of clear correlation between grammatical gender and bias levels suggests that the relationship between language features and model bias is more complex than initially hypothesized.

2. Potential for Unintended Consequences: Users switching between languages may unknowingly encounter varying levels of bias, which could lead to inconsistent or misleading outputs.

3. Need for Language-Specific Strategies: The variation in bias across languages indicates that one-size-fits-all approaches to bias mitigation may be insufficient, necessitating tailored strategies for different linguistic contexts.

### 6.3 Effectiveness of Prompt Engineering

The study investigates the use of gender-neutral language as a potential mitigation strategy:

<quote>As with the direct prompts, the indirect prompts still suffer from significant gender bias. The blue bars are far from 0 (equity) or the random baseline (dashed line). Nonetheless, the measured gender bias is, on average, substantially lower than for the direct prompts.</quote>

These findings reveal:

1. Partial Mitigation: Gender-neutral prompts show some success in reducing bias, indicating that linguistic interventions can have a positive impact.

2. Persistent Challenges: Despite the reduction, significant bias remains even with gender-neutral prompts, suggesting that more comprehensive solutions are needed.

3. Variability in Effectiveness: The impact of gender-neutral prompts varies across languages and models, highlighting the need for nuanced and context-specific approaches to bias mitigation.

### 6.4 Impact on Image Quality

An important consideration in bias mitigation strategies is their potential impact on image quality and relevance:

<quote>In Fig. 6, we can observe that the red bars, i.e. Id, are consistently higher than their respective blue bar, i.e. Ii. This difference shows that neutral formulations lead to an image that is less aligned with the prompt.</quote>

This observation highlights a potential trade-off:

1. Bias vs. Quality: Efforts to reduce bias through linguistic interventions may come at the cost of reduced image quality or relevance.

2. Complexity of Evaluation: Assessing the fairness of T2I models requires considering multiple factors, including both bias levels and output quality.

3. User Experience Implications: The reduced alignment between prompts and generated images when using gender-neutral language could impact user satisfaction and the practical utility of these models in real-world applications.

### 6.5 Out-of-Distribution Languages

The study also examines model performance on languages that were not explicitly part of the training data:

<quote>In Figs. 9 and 10, one can observe that ood languages show a lower MAD score, close to the random baseline. Furthermore, these languages results in worse text-to-image alignment. Both results show that ood languages are not well understood by the model and result in nearly random images, as shown in Fig. 7 (right).</quote>

This finding has important implications:

1. Limitations of Multilingual Models: The poor performance on out-of-distribution languages highlights the boundaries of these models' multilingual capabilities.

2. Risks of Overgeneralization: Users or developers might assume broader language coverage than actually exists, potentially leading to misleading or irrelevant outputs.

3. Need for Explicit Language Support: The results emphasize the importance of carefully considering and explicitly supporting target languages in model development and deployment.

### 6.6 Implications for Model Development and Deployment

The comprehensive analysis presented in this section leads to several important considerations for the development and deployment of multilingual T2I models:

1. Holistic Evaluation: Assessing these models requires considering multiple factors, including bias levels, image quality, and performance across different languages.

2. Tailored Mitigation Strategies: Given the inconsistency of bias across languages, effective bias mitigation may require language-specific approaches.

3. Transparency and User Guidance: Clear communication about model limitations, supported languages, and potential biases is crucial for responsible deployment of these technologies.

4. Ongoing Research and Development: The persistent nature of bias across models and languages underscores the need for continued innovation in model architectures, training techniques, and bias mitigation strategies.

5. Ethical Considerations: The potential for these models to reinforce or amplify gender stereotypes across different cultural contexts raises important ethical questions that must be addressed as part of the development and deployment process.

In conclusion, this large-scale analysis of gender bias in multilingual T2I models reveals a complex landscape of challenges and opportunities. While the persistence of bias across languages is concerning, the variations observed and the partial success of some mitigation strategies provide valuable insights for future research and development efforts aimed at creating more equitable and inclusive AI technologies.

## 7. Discussion and Implications

The findings presented in this study have far-reaching implications for the development, deployment, and societal impact of multilingual text-to-image (T2I) models. This section delves into the key discussion points raised by the authors, exploring the challenges, limitations, and potential future directions for research in this field.

### 7.1 Limitations of Prompt Engineering

One of the central insights of the study is the limited effectiveness of prompt engineering as a bias mitigation strategy:

<quote>Overall, our results suggest that such rewriting is insufficient. Furthermore, they are tedious to obtain on a large scale as it is difficult to find matching phrases. And, they are difficult to formulate for people who are not fluent in the target language.</quote>

This observation has several important implications:

1. Complexity of Bias: The persistence of bias even with carefully crafted gender-neutral prompts suggests that the issue is deeply ingrained in the models' training data and architecture, not easily resolved through surface-level linguistic interventions.

2. Scalability Challenges: The difficulty in creating effective gender-neutral prompts across multiple languages highlights the scalability issues in addressing bias in multilingual models.

3. Accessibility Concerns: The requirement for nuanced linguistic knowledge to craft effective prompts raises questions about the accessibility of bias mitigation strategies for non-expert users.

4. Need for Systemic Approaches: The limitations of prompt engineering underscore the need for more fundamental solutions, potentially involving changes to model architectures, training data curation, and learning algorithms.

### 7.2 Trade-offs in Accessibility and Bias

The study raises important questions about the balance between increasing accessibility to AI technologies and the potential for amplifying biases:

<quote>While these models show remarkable new possibilities for individuals all over the world to participate in the world economy and research, they also bring further bias and discrimination to them. This naturally raises the question whether this technology is beneficial to the social good, i.e. whether its promises outweigh its perils.</quote>

This dilemma touches on several key points:

1. Democratization of AI: Multilingual T2I models have the potential to make powerful creative tools accessible to a global audience, potentially fostering innovation and cultural exchange.

2. Amplification of Stereotypes: However, the biases exhibited by these models could reinforce and amplify existing gender stereotypes across different cultural contexts.

3. Ethical Responsibility: Developers and deployers of these technologies must grapple with the ethical implications of disseminating potentially biased AI systems to a global user base.

4. Need for Inclusive Development: The findings underscore the importance of involving diverse perspectives in the development and evaluation of AI technologies to better understand and address potential biases.

### 7.3 Limitations in Gender Representation

The authors acknowledge an important limitation in their approach to gender classification:

<quote>Unfortunately, all available automated measures treat gender as a binary-valued attribute, though it is not in reality (Wickham, van Nunspeet, and Ellemers 2023; QueerInAI et al. 2023). That said, for this study we use automated gender recognition only on generated images of non-existent people.</quote>

This limitation raises several important considerations:

1. Oversimplification of Gender: The binary classification of gender fails to capture the full spectrum of gender identities, potentially reinforcing restrictive gender norms.

2. Methodological Challenges: The lack of readily available tools for more nuanced gender classification highlights a broader challenge in AI research and development.

3. Ethical Considerations: While the use of automated classification on generated images mitigates some ethical concerns, it still raises questions about the perpetuation of binary gender categorizations in AI systems.

4. Need for Inclusive Metrics: The development of more inclusive and nuanced measures of gender representation is crucial for advancing fairness in AI technologies.

### 7.4 Grammatical Gender and Bias

The study highlights the complex relationship between grammatical gender in languages and bias in T2I models:

<quote>When formulating indirect prompts, many of the languages (cf. Tab. 1) under investigation have a grammatical gender that applies to the neutral phrasing. For example, eine Person (German) has feminine grammatical gender despite being the social neutrum. Hence, for these languages, it is not possible to entirely remove (grammatical) gender.</quote>

This observation underscores several important points:

1. Linguistic Complexity: The interaction between grammatical gender and social gender concepts adds a layer of complexity to bias mitigation efforts in multilingual contexts.

2. Limitations of Neutrality: The impossibility of completely removing gender from certain languages challenges simplistic approaches to creating "neutral" prompts.

3. Cultural Considerations: The varying treatment of gender across languages reflects broader cultural differences that must be considered in the development of global AI technologies.

4. Need for Interdisciplinary Approaches: Addressing these challenges requires collaboration between AI researchers, linguists, and social scientists to develop nuanced, culturally-informed strategies for bias mitigation.

### 7.5 Future Directions

The authors propose several promising avenues for future research:

<quote>Based on these findings, a promising avenue for future research is the improvement of tokenizers by, e.g., learning a gender-neutral token such as "*in" for German, or a general token for all languages. Furthermore, current datasets can be augmented or rephrased with more gender-neutral language by, e.g., adding more nouns with "*in" to the train data or rephrasing existing nouns.</quote>

These suggestions highlight several key areas for development:

1. Enhanced Tokenization: Developing more sophisticated tokenization methods that can better handle gender-neutral language constructs across different languages.

2. Data Augmentation: Creating more diverse and balanced training datasets that include a wider range of gender representations and neutral language formulations.

3. Cross-lingual Learning: Exploring techniques for transferring gender-neutral representations across languages to improve multilingual fairness.

4. Model Architecture Innovations: Investigating new model architectures that can more effectively separate semantic content from gender information in internal representations.

5. Interpretability and Explainability: Developing tools and techniques to better understand how T2I models process and represent gender information, to inform more targeted bias mitigation strategies.

### 7.6 Broader Implications for AI Ethics and Governance

The study's findings have significant implications for the broader field of AI ethics and governance:

1. Global AI Governance: The cross-linguistic nature of bias in these models underscores the need for international collaboration in developing standards and guidelines for fair AI systems.

2. Intersectionality in AI Fairness: The interaction between gender bias and linguistic features highlights the importance of considering intersectional factors in AI fairness research and policy.

3. Transparency and Accountability: The complexity of bias manifestation across languages emphasizes the need for greater transparency from AI developers about the limitations and potential biases of their models.

4. Education and Awareness: There is a crucial need for educating users, developers, and policymakers about the nuanced ways in which bias can manifest in multilingual AI systems.

5. Ethical AI Development: The study reinforces the importance of integrating ethical considerations throughout the AI development lifecycle, from data collection to model deployment and monitoring.

In conclusion, this discussion section highlights the multifaceted challenges involved in addressing gender bias in multilingual T2I models. It underscores the need for holistic, interdisciplinary approaches that consider linguistic, cultural, and technical factors in the pursuit of more equitable and inclusive AI technologies. The insights provided offer valuable direction for future research and development efforts aimed at realizing the potential of these powerful tools while mitigating their potential for harm.

## 8. Conclusion and Future Directions

The study on gender bias in multilingual text-to-image (T2I) models presents a comprehensive analysis of a critical issue in AI development and deployment. The authors summarize their key findings and emphasize the importance of continued research in this area:

<quote>In this work, we investigated gender bias for multilingual T2I models. To this end, we proposed a novel benchmark, MAGBIG with 3630 diverse prompts across nine languages. We evaluated two contemporary T2I models and showed these models to suffer similarly from (gender) biases as their monolingual counterparts. Moreover, we observed these models to perform inconsistently across languages, and indirect gender-neutral prompts could neither resolve this misalignment nor biases.</quote>

This conclusion encapsulates several crucial points:

1. Persistence of Bias: The study demonstrates that gender bias is a pervasive issue in T2I models, extending beyond monolingual contexts to multilingual applications.

2. Cross-linguistic Variation: The inconsistent performance across languages highlights the complex interaction between linguistic features and model biases.

3. Limitations of Current Mitigation Strategies: The ineffectiveness of gender-neutral prompts in fully resolving bias underscores the need for more sophisticated approaches to bias mitigation.

4. Value of Comprehensive Benchmarks: The development of MAGBIG represents a significant contribution to the field, providing a robust tool for evaluating gender bias in multilingual contexts.

The authors call for further research and development in several key areas:

<quote>Consequently, this work calls for more research into fair and diverse representations across languages in image generators. Moreover, we hope future work will employ MAGBIG to rigorously assess T2I models for gender bias in a multilingual setting, ultimately producing fairer AI models.</quote>

This call to action emphasizes several important directions for future work:

1. Improved Representation Learning: Developing techniques to learn fair and diverse representations that generalize across languages and cultures.

2. Advanced Evaluation Methodologies: Refining and expanding benchmarks like MAGBIG to capture more nuanced aspects of bias and fairness in multilingual contexts.

3. Innovative Bias Mitigation Strategies: Exploring novel approaches to reducing bias that go beyond simple prompt engineering, potentially involving architectural changes or advanced training techniques.

4. Cross-disciplinary Collaboration: Encouraging collaboration between AI researchers, linguists, social scientists, and ethicists to develop holistic approaches to addressing bias in AI systems.

5. Ethical AI Development Frameworks: Creating comprehensive frameworks that integrate fairness considerations throughout the AI development lifecycle, from data collection to model deployment and monitoring.

6. User-centric Fairness: Investigating how bias in T2I models impacts end-users across different linguistic and cultural contexts, and developing user-centric approaches to mitigating harmful effects.

7. Transparency and Explainability: Advancing techniques for making the decision-making processes of T2I models more transparent and interpretable, particularly with respect to gender representation.

8. Adaptive Bias Mitigation: Exploring the potential for developing adaptive systems that can adjust their outputs based on detected biases or user feedback in real-time.

9. Long-term Impact Studies: Conducting longitudinal research to understand the societal impacts of deploying biased T2I models across different cultural contexts over time.

10. Policy and Governance: Investigating the legal and policy implications of biased AI systems in a global context, and developing guidelines for responsible development and deployment of multilingual AI technologies.

The conclusion of this study serves not only as a summary of findings but also as a call to action for the AI research community. It highlights the urgent need for continued innovation in creating fair and inclusive AI systems that can serve diverse global populations without perpetuating or amplifying harmful stereotypes.

By emphasizing the complexity of the challenge and the limitations of current approaches, the authors set the stage for a new wave of research that takes a more nuanced, culturally-informed approach to addressing bias in AI. The development of MAGBIG as a comprehensive benchmark represents a significant step forward, providing researchers with a powerful tool for evaluating and improving the fairness of multilingual T2I models.

Ultimately, the goal is to harness the immense potential of these technologies to foster creativity, communication, and cultural exchange on a global scale, while carefully mitigating their potential for harm. This study makes a valuable contribution to that ongoing effort, laying the groundwork for more equitable and responsible AI development in the years to come.

## 9. Broader Impact and Ethical Considerations

The study on gender bias in multilingual text-to-image (T2I) models raises significant ethical considerations and has broad societal implications. The authors provide a thoughtful discussion of these issues:

<quote>As image generation models become increasingly popular and integrated into our lives, fairness must be kept in mind. These models are used in advertisement or design and even come into play in high-stakes applications such as medicine and drug development (Watson et al. 2022). The impact of biased outputs from generative models can be especially detrimental in cross-cultural interactions, where the likelihood of misunderstanding and misinterpretation increases significantly.</quote>

This statement highlights several crucial points regarding the broader impact of biased T2I models:

1. Pervasive Integration: As T2I models become more widely adopted across various domains, their potential impact on society grows exponentially. This integration makes addressing bias a matter of urgent importance.

2. High-Stakes Applications: The use of these models in critical fields such as medicine and drug development underscores the potential real-world consequences of biased outputs. Inaccurate or stereotypical representations could lead to misdiagnoses, ineffective treatments, or other serious outcomes.

3. Cross-Cultural Implications: The multilingual nature of these models means that biases can have far-reaching effects across different cultures and societies. Misrepresentations or stereotypes perpetuated by these models could exacerbate cultural misunderstandings and reinforce harmful prejudices.

4. Economic and Social Influence: The use of T2I models in advertising and design can shape public perceptions and influence consumer behavior. Biased outputs in these domains could contribute to the reinforcement of gender stereotypes in society at large.

The authors further elaborate on the potential negative consequences:

<quote>Consequently, biased language generation can contribute to the spread of misinformation, misrepresentation of diverse cultures, and further marginalization of underrepresented communities in different linguistic contexts. This way, generative models can have a crucial impact on societies and how we include and value diversity in them.</quote>

This statement highlights additional ethical concerns:

5. Misinformation and Stereotyping: Biased T2I models can inadvertently contribute to the spread of misinformation by generating images that reinforce inaccurate stereotypes or misrepresent certain groups.

6. Cultural Misrepresentation: The global reach of these models means they have the power to shape perceptions of different cultures. Biased outputs could lead to oversimplified or inaccurate representations of diverse cultural practices and norms.

7. Marginalization of Underrepresented Groups: By consistently generating images that adhere to dominant stereotypes, these models risk further marginalizing already underrepresented communities, potentially exacerbating existing social inequalities.

8. Shaping Societal Values: The widespread use of these models in various media can influence how society perceives and values diversity. Biased outputs could hinder progress towards more inclusive and equitable social norms.

The authors emphasize the need for ongoing research and vigilance:

<quote>We hope our work and benchmark contribute to a greater focus on gender bias in generative AI models, particularly across languages.</quote>

This call to action underscores several important points:

9. Continuous Evaluation: As T2I models evolve and improve, there is a need for ongoing evaluation and monitoring of bias across different languages and cultural contexts.

10. Interdisciplinary Collaboration: Addressing these complex ethical issues requires collaboration between AI researchers, ethicists, linguists, sociologists, and policymakers.

11. Proactive Approach: Rather than waiting for problems to emerge, the AI community should take a proactive stance in identifying and mitigating potential biases before models are widely deployed.

12. Global Perspective: The multilingual focus of this research highlights the importance of considering diverse global perspectives in AI ethics and governance.

Additional ethical considerations that arise from this study include:

13. Transparency and Accountability: There is a need for greater transparency from AI developers about the limitations and potential biases of their models, as well as clear accountability mechanisms for addressing harmful outputs.

14. User Awareness and Education: As these models become more accessible to the general public, there is a responsibility to educate users about potential biases and how to interpret and use the generated images responsibly.

15. Regulatory Implications: The findings of this study may have implications for AI regulation, potentially calling for standards or guidelines around bias testing and mitigation in multilingual AI systems.

16. Ethical AI Development Practices: The study underscores the importance of integrating ethical considerations throughout the AI development lifecycle, from data collection and model training to deployment and monitoring.

17. Long-term Societal Impact: There is a need to consider the long-term effects of deploying biased AI systems on societal norms, cultural perceptions, and global communication.

18. Balancing Innovation and Caution: While addressing bias is crucial, it's also important to balance these concerns with the potential benefits of T2I technologies in fostering creativity, cross-cultural understanding, and accessibility.

19. Intersectionality: The study's focus on gender bias raises questions about how other forms of bias (e.g., racial, age-related, or socioeconomic) might intersect with gender in multilingual contexts.

20. Ethical Use of Benchmarks: While MAGBIG provides a valuable tool for assessing bias, there is an ethical responsibility to use such benchmarks thoughtfully and avoid over-simplifying complex societal issues.

In conclusion, the broader impact and ethical considerations raised by this study are far-reaching and multifaceted. They touch on issues of fairness, representation, cultural sensitivity, and the responsible development of AI technologies. By highlighting these concerns, the authors contribute to an ongoing and crucial dialogue about the role of AI in shaping our global society and the ethical imperatives that must guide its development and deployment. The study serves as a call to action for the AI community to prioritize fairness and inclusivity in the pursuit of technological advancement, ensuring that the benefits of these powerful tools are realized without exacerbating existing societal inequities.

## 10. Acknowledgments

The acknowledgments section of a research paper is crucial as it recognizes the support, contributions, and funding that made the study possible. In this case, the authors express their gratitude to various institutions and funding sources:

<quote>We gratefully acknowledge support by the German Center for Artificial Intelligence (DFKI) project "SAINT", the Federal Ministry of Education and Research (BMBF) project "AISC " (GA No. 01IS22091), and the Hessian Ministry for Digital Strategy and Development (HMinD) project "AI Innovationlab" (GA No. S-DIW04/0013/003). This work also benefited from the ICT-48 Network of AI Research Excellence Center "TAILOR" (EU Horizon 2020, GA No 952215), the Hessian Ministry of Higher Education, and the Research and the Arts (HMWK) cluster projects "The Adaptive Mind" and "The Third Wave of AI".</quote>

This acknowledgment highlights several important aspects of the research ecosystem:

1. Institutional Support: The German Center for Artificial Intelligence (DFKI) is recognized for their support through the "SAINT" project. This indicates the involvement of a major research institution in the field of AI, lending credibility to the study.

2. Government Funding: The Federal Ministry of Education and Research (BMBF) and the Hessian Ministry for Digital Strategy and Development (HMinD) are acknowledged for their financial support. This demonstrates the recognition of the importance of this research at the governmental level.

3. European Union Support: The mention of the ICT-48 Network of AI Research Excellence Center "TAILOR" (EU Horizon 2020) indicates that this research is part of a broader European initiative in AI research.

4. Interdisciplinary Collaboration: The acknowledgment of cluster projects like "The Adaptive Mind" and "The Third Wave of AI" suggests that this research is part of larger, interdisciplinary efforts to advance AI technology and understanding.

5. Transparency in Funding: By clearly stating the grant agreement numbers (e.g., GA No. 01IS22091), the authors provide transparency about the funding sources, which is crucial for assessing potential conflicts of interest and understanding the scale of support.

6. Recognition of Broader Research Ecosystem: The acknowledgment of various ministries and research clusters indicates that this study is part of a larger ecosystem of AI research and development in Germany and Europe.

The acknowledgments section serves several important functions:

- It provides transparency about the sources of funding and support for the research, which is crucial for maintaining scientific integrity and allowing readers to assess potential biases or influences.
- It recognizes the collaborative nature of scientific research, highlighting the various institutions and programs that contribute to advancing knowledge in the field.
- It demonstrates the societal and governmental interest in addressing issues of bias in AI, as evidenced by the support from multiple government ministries and research initiatives.
- It places the research within the context of broader AI research efforts, both nationally and internationally, indicating its relevance to ongoing scientific discourse and policy development.

By acknowledging this wide range of support, the authors not only express gratitude but also validate the importance and relevance of their work within the larger scientific and policy-making communities. This recognition of support and collaboration is an essential aspect of scientific research, fostering transparency, credibility, and continued investment in critical areas of study like AI ethics and fairness.

## 11. References

The references section of this study is crucial as it provides the foundation upon which the research is built and situates the work within the broader scientific context. While the full list of references is not provided in the excerpt, we can infer several key points about the nature and scope of the sources cited:

1. Breadth of Literature: The authors mention drawing inspiration from previous works such as Luccioni et al. (2023) and Friedrich et al. (2023), indicating that they have built upon recent research in the field of AI bias and fairness.

2. Interdisciplinary Approach: The references likely span multiple disciplines, including computer science, linguistics, ethics, and social sciences, reflecting the interdisciplinary nature of the research on bias in AI systems.

3. Foundational Works: The authors cite seminal papers in the field of gender bias in NLP, such as Bolukbasi et al. (2016) and Caliskan, Bryson, and Narayanan (2017), demonstrating their engagement with foundational literature on AI bias.

4. Recent Developments: Citations of works from 2023 (e.g., Bellagente et al. 2023, Ye et al. 2023) indicate that the authors have incorporated the most recent developments in multilingual T2I models into their research.

5. Methodological Sources: References to works on evaluation metrics and methodologies (e.g., Kärkkäinen and Joo 2021 for FairFace, Hessel et al. 2021 for text-to-image alignment) show the authors' commitment to using established and peer-reviewed methods in their study.

6. Ethical and Societal Impact Literature: The inclusion of sources discussing the broader impacts of AI (e.g., Watson et al. 2022 on applications in medicine) demonstrates the authors' consideration of the ethical and societal implications of their work.

7. Linguistic and Cultural Sources: Given the multilingual focus of the study, the references likely include works on grammatical gender across languages and cultural aspects of gender representation.

8. Technical AI Literature: Citations of papers describing the T2I models evaluated (MultiFusion and AltDiffusion) provide the technical background necessary for understanding the systems under study.

9. Bias Mitigation Strategies: The references probably include works on various approaches to mitigating bias in AI systems, including prompt engineering and data augmentation techniques.

10. Benchmarking and Evaluation: Citations of other benchmarks and evaluation frameworks for T2I models likely provide context for the development and significance of MAGBIG.

The comprehensive nature of the references section would typically serve several important functions:

- Establishing Credibility: By citing a wide range of peer-reviewed sources, the authors demonstrate the scholarly rigor of their work.
- Providing Context: The references situate the current study within the existing body of knowledge on AI bias, multilingual models, and image generation.
- Acknowledging Predecessors: Proper citation gives credit to previous researchers whose work has informed or inspired the current study.
- Facilitating Further Research: A thorough reference list allows readers to explore related works and verify the authors' claims.
- Demonstrating Novelty: By comprehensively covering existing literature, the authors can clearly show how their work contributes new knowledge to the field.

In conclusion, while we don't have the full list of references, it's clear that the authors have drawn from a diverse and up-to-date body of literature to support their research. This comprehensive approach to citation enhances the credibility and impact of their study on gender bias in multilingual text-to-image generation models.

## 12. Appendix and Supplementary Materials

The appendix of a research paper often contains additional details, analyses, and explanations that support the main text but are too extensive or technical to include in the body of the paper. Based on the information provided in the excerpt, the appendix for this study on gender bias in multilingual text-to-image (T2I) models includes several important components:

### 12.1 Additional Results

The appendix provides further analysis of the MAGBIG dataset, including:

1. Performance on Out-of-Distribution Languages:
   <quote>In Figs. 9 and 10, one can observe that ood languages show a lower MAD score, close to the random baseline. Furthermore, these languages results in worse text-to-image alignment.</quote>
   This additional analysis helps to understand the limitations of the models when dealing with languages they weren't explicitly trained on.

2. Qualitative Results for Adjective Prompts:
   The appendix likely includes examples and analyses of how the models perform with adjective-based prompts across different languages, providing a more nuanced understanding of bias manifestation.

3. Dis-aggregated/Directed Results:
   <quote>In Fig. 11, we show dis-aggregated/directed results from the main experiments, i.e. instead of computing the (undirected) MAD, we computed now the average bias direction across the occupations.</quote>
   This analysis provides insights into the specific direction of bias (e.g., towards male or female representations) for different occupations and languages.

### 12.2 Details on the "Gender Star" Formulations

The appendix elaborates on the German gender star convention and its implementation in the study:

<quote>The German gender star (Julia Misersky and Snijders 2019) works by splicing feminine and masculine forms into one form, with an asterisk as a separator. There are multiple approaches to the potential grammatical issues this causes when paired with German declension suffixes—or even more noticeably, changing noun stems, as in "Arzt" and "Ärztin" (doctor, m. and doctor, f.).</quote>

This detailed explanation helps readers understand the linguistic complexities involved in creating gender-neutral formulations and the challenges of implementing them in T2I models.

### 12.3 Grammatical Gender in Languages Used

A comprehensive table is provided showing the linguistic properties of grammatical gender in the languages covered by the study:

<quote>Table 3 contains a list of yes-no questions from GramBank (Skirgård et al. 2023), giving a more complete picture of grammatical gender in the languages we use.</quote>

This detailed linguistic information is crucial for understanding how different language structures might influence the manifestation of gender bias in T2I models.

### 12.4 Details on FairFace

The authors describe the validation process for the FairFace classifier used in the study:

<quote>We generated 250 images of persons with varying appearances (gender, age, skin tone, etc.) with SD1.5 and had them labeled by users on thehive.com with sanity checks. We compared those labels with the FairFace labels and found a matching rate of ∼93%, which was similar across all appearance types.</quote>

This validation process provides important context for interpreting the gender classification results used throughout the study.

### 12.5 Translation Pipeline

Information is provided on the process of translating prompts and ensuring consistency across languages:

<quote>When generating prompts we started off with simple LLMs. Yet, the prompts were never consistent across languages which added an unnecessary confounder/noise to our evaluations. With our pipeline, in contrast, the translations were always consistent across languages and occupations.</quote>

This explanation of the translation methodology is crucial for understanding how the multilingual prompts were created and for assessing the reliability of cross-linguistic comparisons.

### 12.6 Image Generation and Out-of-Distribution Languages

The appendix includes details on the image generation process and the handling of out-of-distribution languages:

<quote>As discussed in the main text, generating images for each occupation took usually more attempts than just 100. Specifically for ood languages, the number of attempts became large as the image content seemed random and consequently the prompts were not understood.</quote>

This information provides important context for interpreting the results, particularly for languages that were not part of the models' original training data.

### 12.7 List of Prompt Items

A comprehensive list of the adjectives and occupations used in the MAGBIG benchmark is provided. This list is crucial for understanding the scope and diversity of the prompts used in the study, and for potential replication or extension of the research.

The inclusion of these detailed appendices and supplementary materials serves several important functions:

1. Transparency: By providing detailed methodological information and additional analyses, the authors enhance the transparency and reproducibility of their research.

2. Depth of Analysis: The additional results and breakdowns allow for a more nuanced understanding of the findings, going beyond what could be included in the main text.

3. Linguistic Context: The detailed information on grammatical gender and language-specific features provides crucial context for interpreting the results across different linguistic systems.

4. Methodological Rigor: The validation of tools like FairFace and the explanation of the translation pipeline demonstrate the careful consideration given to methodological choices.

5. Future Research: The comprehensive prompt list and detailed methodologies provide a foundation for future studies to build upon or replicate this work.

6. Interdisciplinary Value: The linguistic details and cross-language comparisons make this appendix valuable not only to AI researchers but also to linguists and social scientists interested in the intersection of language and technology.